services:
  redis:
    image: redis:alpine3.22
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - llm_net

  orchestrator:
    build:
      context: .
      dockerfile: orchestrator/Dockerfile
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
      vllm:
        condition: service_started
    environment:
      ORCH_API_KEY: ${API_KEY}
      ORCH_ENCRYPTION_KEY: ${ENCRYPTION_KEY}
      ORCH_LLM_SERVER_URL: http://vllm:8000/v1/completions
      ORCH_LLM_METRICS_URL: http://vllm:8000/metrics
      ORCH_EMBEDDING_SERVER_URL: http://vllm:8000/v1/embeddings
      ORCH_REDIS_URL: redis://redis:6379/0
      ORCH_ENABLE_REQUEST_LOGS: ${ENABLE_REQUEST_LOGS:-false}
      ORCH_LOG_FILE_PATH: /app/logs/llm_request_logs.jsonl
      ORCH_LOG_LEVEL: ${LOG_LEVEL:-INFO}
    volumes:
      - orchestrator_logs:/app/logs
    ports:
      - "8005:8005"
    networks:
      - llm_net

  vllm:
    image: ghcr.io/vllm-project/vllm-openai:latest
    restart: unless-stopped
    environment:
      HF_TOKEN: ${HF_TOKEN}
      VLLM_API_KEY: ${HF_TOKEN}
      VLLM_DEFAULT_MODEL: ${MODEL_NAME}
    command:
      - "--model"
      - ${MODEL_NAME}
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--tensor-parallel-size"
      - "${TENSOR_PARALLEL:-1}"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    healthcheck:
      test:
        [
          "CMD",
          "python3",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://127.0.0.1:8000/health')",
        ]
      interval: 10s
      timeout: 5s
      retries: 6
    volumes:
      - model_cache:/models
    networks:
      - llm_net

networks:
  llm_net:
    driver: bridge

volumes:
  orchestrator_logs:
  model_cache:

